{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twittercrawler.utils import tweet_time_2_epoch\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetQuery():\n",
    "    def __init__(self, tweet_dict):\n",
    "        self._id = tweet_dict.get(\"id_str\", None)\n",
    "        self._user_name = tweet_dict[\"user\"][\"screen_name\"] if \"user\" in tweet_dict else None\n",
    "        self._user_id = tweet_dict[\"user\"][\"id_str\"] if \"user\" in tweet_dict else None\n",
    "        self._epoch = tweet_time_2_epoch(tweet_dict[\"created_at\"]) if \"created_at\" in tweet_dict else None\n",
    "        self._likes = tweet_dict.get(\"favorite_count\", None)\n",
    "        self._retweets = tweet_dict.get(\"retweet_count\", None)\n",
    "        self._max_id = None\n",
    "        self._since_id = int(self._id) if self._id != None else None\n",
    "        self._last_access = None\n",
    "        \n",
    "    def set_epoch(self, epoch):\n",
    "        self._epoch = epoch\n",
    "        \n",
    "    def set_max_id(self, max_id):\n",
    "        self._max_id = max_id\n",
    "        \n",
    "    def set_since_id(self, since_id):\n",
    "        self._since_id = int(since_id)\n",
    "        \n",
    "    def mark_access(self):\n",
    "        self._last_access = dt.now()\n",
    "        \n",
    "    def copy(self):\n",
    "        params = {\n",
    "            \"id_str\":self.id,\n",
    "            \"favorite_count\":self.likes,\n",
    "            \"retweet_count\":self.retweets,\n",
    "            \"user\":{\n",
    "                \"id_str\": self.user_id,\n",
    "                \"screen_name\": self.user_name\n",
    "            }\n",
    "        }\n",
    "        copy_q = TweetQuery(params)\n",
    "        copy_q.set_epoch(self.epoch)\n",
    "        copy_q.set_max_id(self.max_id)\n",
    "        copy_q.set_since_id(self.since_id)\n",
    "        return copy_q\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"(%s, %s, %s, %s, %i, %i, %s, %s)\" % (self.id, self.user_id, self.user_name, self.dt, self.likes, self.retweets, self.max_id, self.since_id)\n",
    "    \n",
    "    @property\n",
    "    def id(self):\n",
    "        return self._id\n",
    "    \n",
    "    @property\n",
    "    def user_name(self):\n",
    "        return self._user_name\n",
    "    \n",
    "    @property\n",
    "    def user_id(self):\n",
    "        return self._user_id\n",
    "    \n",
    "    @property\n",
    "    def epoch(self):\n",
    "        return self._epoch\n",
    "    \n",
    "    @property\n",
    "    def dt(self):\n",
    "        return dt.fromtimestamp(self.epoch)\n",
    "    \n",
    "    @property\n",
    "    def likes(self):\n",
    "        return self._likes\n",
    "    \n",
    "    @property\n",
    "    def retweets(self):\n",
    "        return self._retweets\n",
    "    \n",
    "    @property\n",
    "    def max_id(self):\n",
    "        return self._max_id\n",
    "    \n",
    "    @property\n",
    "    def since_id(self):\n",
    "        return self._since_id\n",
    "    \n",
    "    @property\n",
    "    def elapsed_days(self):\n",
    "        return (dt.now() - self.dt).days\n",
    "    \n",
    "    @property\n",
    "    def accessed_since_days(self):\n",
    "        if self._last_access == None:\n",
    "            return -1\n",
    "        else:\n",
    "            return (dt.now() - self._last_access).days\n",
    "        \n",
    "    @property\n",
    "    def priority(self):\n",
    "        if self.max_id != None:\n",
    "            return 10.0\n",
    "        elif self.accessed_since_days == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return self.elapsed_days + np.log10(1.0+self.likes+self.retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, shutil\n",
    "\n",
    "class UserTweetStore():\n",
    "    def __init__(self, store_dir):\n",
    "        self.store_dir = store_dir\n",
    "        self._user_intervals = {}\n",
    "        \n",
    "    @property\n",
    "    def store_dir(self):\n",
    "        return self._store_dir\n",
    "    \n",
    "    @property\n",
    "    def user_intervals(self):\n",
    "        return self._user_intervals\n",
    "    \n",
    "    @store_dir.setter\n",
    "    def store_dir(self, val):\n",
    "        if not os.path.exists(val):\n",
    "            os.makedirs(val)\n",
    "        self._store_dir = val\n",
    "        \n",
    "    @property\n",
    "    def interval_file(self):\n",
    "        return os.path.join(self.store_dir, \"user_intervals.json\")\n",
    "        \n",
    "    @property\n",
    "    def replies_dir(self):\n",
    "        rdir = os.path.join(self.store_dir, \"replies\")\n",
    "        if not os.path.exists(rdir):\n",
    "            os.makedirs(rdir)\n",
    "        return rdir\n",
    "        \n",
    "    def save(self):\n",
    "        with open(self.interval_file, 'w') as f:\n",
    "            json.dump(self._user_intervals, f, indent=4)\n",
    "        \n",
    "    def load(self):\n",
    "        with open(self.interval_file) as f:\n",
    "            self._user_intervals = json.load(f)\n",
    "            \n",
    "    def reset(self):\n",
    "        print(\"UserStore reset in this directory: %s\" % self.store_dir)\n",
    "        if os.path.exists(self.store_dir):\n",
    "            shutil.rmtree(self.store_dir)\n",
    "        self._user_intervals = {}\n",
    "    \n",
    "    def get_user(self, user_id):\n",
    "        return self._user_intervals.get(user_id, [None, None])\n",
    "    \n",
    "    def update(self, query, latest_id):\n",
    "        user_id = query.user_id\n",
    "        from_id, to_id = self.get_user(user_id)\n",
    "        qid = int(query.id)\n",
    "        if from_id == None or qid < from_id:\n",
    "            from_id = qid\n",
    "        if to_id == None or latest_id > to_id:\n",
    "            to_id = latest_id\n",
    "        self._user_intervals[user_id] = [from_id, to_id]\n",
    "    \n",
    "    def adjust_query(self, query):\n",
    "        from_id, to_id = self.get_user(query.user_id)\n",
    "        queries = []\n",
    "        if from_id == None:\n",
    "            queries.append(query)\n",
    "        else:\n",
    "            if query.since_id < from_id:\n",
    "                q = query.copy()\n",
    "                q.set_max_id(from_id)\n",
    "            # avoid collecting the same data twice\n",
    "            query.set_since_id(to_id)\n",
    "            queries.append(query)\n",
    "        return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twittercrawler.crawlers import RecursiveCrawler\n",
    "crawler = RecursiveCrawler()\n",
    "crawler.authenticate(\"../api_key.json\")\n",
    "status = crawler.twitter_api.show_status(id=\"1347260116932976643\", tweet_mode=\"extended\")\n",
    "q = TweetQuery(status)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = UserTweetStore(\"alma\")\n",
    "store.update(q, q.since_id+10000000)\n",
    "store.save()\n",
    "store.user_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_tmp = UserTweetStore(\"alma\")\n",
    "store_tmp.load()\n",
    "assert store_tmp.user_intervals == store.user_intervals\n",
    "store_tmp.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twittercrawler.utils import load_json_result\n",
    "    \n",
    "class SearchEngine():\n",
    "    def __init__(self, crawler, store, tweet_mode=\"extended\"):\n",
    "        self.crawler = crawler\n",
    "        self.store = store\n",
    "        self.tweet_mode = tweet_mode\n",
    "        \n",
    "    def get_status(self, tweet_id):\n",
    "        return self.crawler.twitter_api.show_status(id=tweet_id, tweet_mode=self.tweet_mode)\n",
    "    \n",
    "    def get_output_fp(self, query):\n",
    "        return os.path.join(self.store.replies_dir,\"%s.txt\" % query.user_id)\n",
    "        \n",
    "    def collect_replies(self, query, count=100, result_type='recent', wait_for=3, feedback_time=15):\n",
    "        search_params = {\n",
    "            \"q\" : \"to:%s\" % query.user_name,\n",
    "            \"result_type\" : result_type,\n",
    "            \"count\" : count,\n",
    "            \"tweet_mode\" : self.tweet_mode\n",
    "        }\n",
    "        self.crawler.connect_to_file(self.get_output_fp(query))\n",
    "        self.crawler.set_search_arguments(search_args=search_params)\n",
    "        if query.max_id != None:\n",
    "            result = self.crawler.search(wait_for=wait_for, feedback_time=feedback_time, current_max_id=query.max_id, custom_since_id=query.since_id)\n",
    "        else:\n",
    "            result = self.crawler.search(wait_for=wait_for, feedback_time=feedback_time, custom_since_id=query.since_id)\n",
    "        self.crawler.close()\n",
    "        return result\n",
    "    \n",
    "    def extract_replies(self, query):\n",
    "        reply_tweets = load_json_result(self.get_output_fp(query))\n",
    "        replies = []\n",
    "        for tweet in reply_tweets:\n",
    "            replied_tweet = tweet.get(\"in_reply_to_status_id_str\", None)\n",
    "            if replied_tweet == query.id:\n",
    "                replies.append(tweet)\n",
    "        return replies\n",
    "        \n",
    "    # TODO: nagyon átgondolni!!!\n",
    "    def execute(self, original_query):\n",
    "        queries = self.store.adjust_query(original_query)\n",
    "        for idx, query in enumerate(queries):\n",
    "            success, max_id, latest_id, cnt = self.collect_replies(query)\n",
    "            print(idx, success, max_id, latest_id, cnt)\n",
    "            if success:\n",
    "                if cnt > 0:\n",
    "                    self.store.update(query, latest_id)\n",
    "            else:\n",
    "                break\n",
    "        replies = []\n",
    "        if success:\n",
    "            replies = self.extract_replies(original_query)\n",
    "            original_query.mark_access()\n",
    "            if cnt > 0:\n",
    "                original_query.set_since_id(latest_id)\n",
    "                original_query.set_max_id(None)\n",
    "        else:\n",
    "            original_query.set_max_id(max_id)\n",
    "        return success, original_query, replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = UserTweetStore(\"test\")\n",
    "engine = SearchEngine(crawler, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplyCollector():\n",
    "    def __init__(self, engine, tweet_id, reset=False, likes_limit=5, postpone_day_limit=3, action_day_limit=5):\n",
    "        self.engine = engine\n",
    "        self.tweet_id = tweet_id\n",
    "        self.reset = reset\n",
    "        self.likes_limit = likes_limit\n",
    "        self.postpone_day_limit = postpone_day_limit\n",
    "        self.action_day_limit = action_day_limit\n",
    "        if self.reset:\n",
    "            #self.init()\n",
    "            self.engine.store.reset()\n",
    "        self.seed_tweet = self.engine.get_status(self.tweet_id)\n",
    "        self.tweet_thread = [self.seed_tweet]\n",
    "        print(self.seed_tweet['full_text'])\n",
    "        self._queue = deque([TweetQuery(self.seed_tweet)])\n",
    "        self.active_tweet_ids = []\n",
    "        #else:\n",
    "        #    raise NotImplementedError(\"TODO: collector loading and saving functionalities!!!\")\n",
    "        \n",
    "    #def init(self):\n",
    "    #    self.engine.store.reset()\n",
    "    #    self.seed_tweet = self.engine.get_status(self.tweet_id)\n",
    "    #    self.tweet_thread = [self.seed_tweet]\n",
    "    #    print(self.seed_tweet['full_text'])\n",
    "    #    self._queue = deque([TweetQuery(self.seed_tweet)])\n",
    "    #    self.active_tweet_ids = []\n",
    "        \n",
    "    @property\n",
    "    def queue(self):\n",
    "        return self._queue\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.queue)\n",
    "    \n",
    "    @property\n",
    "    def active_queries(self):\n",
    "        return [q for q in self.queue if q.priority > 0]\n",
    "    \n",
    "    @property\n",
    "    def status(self):\n",
    "        self.engine.store.save()\n",
    "        return {\n",
    "            \"total_queries\": self.size,\n",
    "            \"remaining queries\": len(self.active_queries),\n",
    "            \"seed_tweet_id\": self.tweet_id,\n",
    "        }\n",
    "    \n",
    "    def _sort_queries(self):\n",
    "        self._queue = deque(sorted(self._queue, key=lambda x: x.priority, reverse=True))\n",
    "        \n",
    "    def _decide_execution(self, query):\n",
    "        if query.elapsed_days < self.postpone_day_limit:\n",
    "            execute_now = False\n",
    "        elif query.elapsed_days >= self.action_day_limit:\n",
    "            # here we try to catch the begining of each thread\n",
    "            execute_now = True\n",
    "        else:\n",
    "            if query.likes >= self.likes_limit:\n",
    "                execute_now = True\n",
    "            else:\n",
    "                execute_now = False\n",
    "        return execute_now\n",
    "        \n",
    "    def run(self, feedback_interval=10, max_requests=1000):\n",
    "        i = 0\n",
    "        while len(self.queue) > 0:\n",
    "            query = self.queue.popleft()\n",
    "            print(query)\n",
    "            if query.priority == 0:\n",
    "                self._queue.appendleft(query)\n",
    "                break\n",
    "            execute_now = self._decide_execution(query)\n",
    "            if execute_now:\n",
    "                success, new_query, replies = self.engine.execute(query)\n",
    "                print(query.id,len(replies))\n",
    "                self.tweet_thread += replies\n",
    "                df = pd.DataFrame(self.tweet_thread)\n",
    "                df.to_pickle(\"%s.pkl\" % self.tweet_id)\n",
    "                for reply in replies:\n",
    "                    q = TweetQuery(reply)\n",
    "                    if not q.id in self.active_tweet_ids:\n",
    "                        self._queue.append(q)\n",
    "                self._queue.append(new_query)\n",
    "                self._sort_queries()\n",
    "                if not success:\n",
    "                    break\n",
    "            else:\n",
    "                self._queue.append(query)\n",
    "            i += 1\n",
    "            if i % feedback_interval == 0:\n",
    "                print(\"\\n### STATUS ###\")\n",
    "                print(self.status)\n",
    "                print()\n",
    "            if i >= max_requests:\n",
    "                print(\"Exiting at %i executed queries!\" % max_requests)\n",
    "        print(self.status)\n",
    "        # TODO: define conditions for dropping queries from the queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- **export / load functions for store, collector etc.:**\n",
    "   - store: save user intervals\n",
    "   - collector: save tweet_thread\n",
    "- records are never deleted from the queue!!!\n",
    "- likes_limit=5 => interaction_limit = likes+retweets **(maybe we should update each query status at processing time!!!)** - to know the actual engagement counts\n",
    "- write a few tests...\n",
    "- include comet logging..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://twitter.com/hadleywickham/status/1347260334227259394\n",
    "collector = ReplyCollector(engine, \"1347260116932976643\", reset=True, likes_limit=1, postpone_day_limit=0, action_day_limit=1)\n",
    "collector.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://twitter.com/nicvadivelu/status/1347267259266318336/photo/1\n",
    "collector = ReplyCollector(engine, \"1347267259266318336\", likes_limit=1, postpone_day_limit=0, action_day_limit=0, reset=True)\n",
    "collector.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://twitter.com/Dr2NisreenAlwan/status/1347833203080523776\n",
    "collector = ReplyCollector(engine, \"1347833203080523776\", likes_limit=5, postpone_day_limit=0, action_day_limit=5, reset=False)\n",
    "collector.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://twitter.com/indepdubnrth/status/1347979792839274499\n",
    "collector = ReplyCollector(engine, \"1347979792839274499\", likes_limit=5, postpone_day_limit=0, action_day_limit=5, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://twitter.com/RandPaul/status/1347930653904556036\n",
    "collector = ReplyCollector(engine, \"1347930653904556036\", likes_limit=5, postpone_day_limit=0, action_day_limit=5, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: stop after executing a few queries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_id = \"1347260116932976643\"\n",
    "tweet_id = \"1347267259266318336\"\n",
    "#tweet_id = \"1347833203080523776\"\n",
    "#tweet_id = \"1347979792839274499\"\n",
    "#tweet_id = \"1347930653904556036\"\n",
    "thread_df = pd.read_pickle(\"%s.pkl\" % tweet_id)\n",
    "repliers = set()\n",
    "edges = []\n",
    "mentions = []\n",
    "#thread_events = sorted(collector.tweet_thread, key=lambda x: x[\"id\"])\n",
    "#for tw in thread_events:\n",
    "for _, tw in thread_df.iterrows():\n",
    "    if tw[\"in_reply_to_status_id_str\"] != None:\n",
    "        edges.append((tw[\"id_str\"],tw[\"in_reply_to_status_id_str\"]))\n",
    "        mentions.append((tw[\"user\"][\"id_str\"],tw[\"in_reply_to_user_id_str\"]))\n",
    "    #repliers.add(tw[\"user\"][\"screen_name\"])\n",
    "    #print(tw[\"user\"][\"screen_name\"], tw[\"created_at\"])\n",
    "    #print(tw[\"full_text\"])\n",
    "    #print()\n",
    "print(len(edges), len(collector.queue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.DiGraph()\n",
    "H = nx.MultiDiGraph()\n",
    "G.add_edges_from(edges)\n",
    "_ = H.add_edges_from(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(H, node_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
